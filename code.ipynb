{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pdogg Windows10\\Desktop\\Semester 7\\Computer Vision\\Reinforcement-Learning-Enhanced-Visual-Storytelling\\cv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pdogg Windows10\\Desktop\\Semester 7\\Computer Vision\\Reinforcement-Learning-Enhanced-Visual-Storytelling\\cv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organising the Image-Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_folder = 'Dataset/Organized_Annotations/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'Dataset/SSID_Images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset/Organized_Annotations/SSID_Train_Organized.json') as f:\n",
    "    organized_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_data = {}\n",
    "\n",
    "album_limit = 100\n",
    "processed_album_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for album_id in organized_data.items():\n",
    "    if processed_album_count >= album_limit:\n",
    "        break  # Stop after processing 100 albums\n",
    "\n",
    "    album_id = album_id[1]\n",
    "\n",
    "    # get the value of the first key in the dictionary\n",
    "    stories = album_id[list(album_id.keys())[0]]\n",
    "\n",
    "    for item in stories:\n",
    "\n",
    "        # add a key value pair to the dictionary, key being the image_id and value being the storytext\n",
    "        fine_tune_data[item['image_id']] = item['storytext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625\n"
     ]
    }
   ],
   "source": [
    "print(len(fine_tune_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(fine_tune_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_fine_tune_data = dict(sorted(fine_tune_data.items(), key=lambda x: int(x[0])))\n",
    "\n",
    "#print(sorted_fine_tune_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, data_dict, image_folder, processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dict (dict): A dictionary where keys are image names, and values are captions.\n",
    "            image_folder (str): Path to the folder containing the images.\n",
    "            processor (BlipProcessor): The processor to preprocess the images and captions.\n",
    "        \"\"\"\n",
    "        self.data_dict = data_dict\n",
    "        self.image_folder = image_folder\n",
    "        self.processor = processor\n",
    "        self.image_keys = list(data_dict.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_keys)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_key = self.image_keys[idx]\n",
    "        caption = self.data_dict[image_key]\n",
    "        \n",
    "        # Load the image\n",
    "        image_path = f\"{self.image_folder}/{image_key}.jpg\"  # Assumes image is stored as jpg\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Preprocess the image and caption\n",
    "        inputs = self.processor(images=image, text=caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs['input_ids'].squeeze(),  # Caption tokens\n",
    "            \"attention_mask\": inputs['attention_mask'].squeeze(),  # Attention mask for the caption\n",
    "            \"pixel_values\": inputs['pixel_values'].squeeze()  # Processed image tensor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only first 100 images for fine tuning\n",
    "sorted_fine_tune_data = dict(list(sorted_fine_tune_data.items())[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted_fine_tune_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pdogg Windows10\\Desktop\\Semester 7\\Computer Vision\\Reinforcement-Learning-Enhanced-Visual-Storytelling\\cv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 13.1148042678833\n",
      "Epoch 2/3\n",
      "Loss: 11.080038070678711\n",
      "Epoch 3/3\n",
      "Loss: 10.039155006408691\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "epochs = 3\n",
    "learning_rate = 5e-5\n",
    "batch_size = 8\n",
    "\n",
    "# Create the dataset and data loader\n",
    "dataset = ImageCaptionDataset(sorted_fine_tune_data, image_folder, processor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set the model in training mode\n",
    "model.train()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        # Move the data to the GPU if available\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, pixel_values=pixel_values, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = []\n",
    "image_paths = []\n",
    "\n",
    "# Load the images\n",
    "for i in range(6, 11):\n",
    "    image_path = f\"{image_folder}/{i}.jpg\"\n",
    "    image_paths.append(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dataset/SSID_Images//6.jpg', 'Dataset/SSID_Images//7.jpg', 'Dataset/SSID_Images//8.jpg', 'Dataset/SSID_Images//9.jpg', 'Dataset/SSID_Images//10.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_images = [preprocess_image(image_path) for image_path in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = []\n",
    "\n",
    "for inputs in processed_images:\n",
    "    outputs = model.generate(**inputs)\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption for image 1: a man in a blue jacket and helmet walks through a narrow ravine\n",
      "Caption for image 2: a horse in a field with the words,'the best horse is a horse '\n",
      "Caption for image 3: a man and woman walking down a dirt road\n",
      "Caption for image 4: the cover of the book, the book of the year, with a photo of a mountain range\n",
      "Caption for image 5: a man in a blue shirt and hat walks down the street\n"
     ]
    }
   ],
   "source": [
    "for i, caption in enumerate(captions):\n",
    "    print(f\"Caption for image {i+1}: {caption}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pdogg Windows10\\Desktop\\Semester 7\\Computer Vision\\Reinforcement-Learning-Enhanced-Visual-Storytelling\\cv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, CLIPProcessor, CLIPModel, GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_folder = 'Dataset/Organized_Annotations/'\n",
    "image_folder = 'Dataset/SSID_Images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "for i in range(1, 11):\n",
    "    image_path = f\"{image_folder}/{i}.jpg\"\n",
    "    image_paths.append(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pdogg Windows10\\Desktop\\Semester 7\\Computer Vision\\Reinforcement-Learning-Enhanced-Visual-Storytelling\\cv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs)\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pdogg Windows10\\Desktop\\Semester 7\\Computer Vision\\Reinforcement-Learning-Enhanced-Visual-Storytelling\\cv\\lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "image_captions = {image_path: generate_caption(image_path) for image_path in image_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/SSID_Images//1.jpg: a group of people walking up a snowy slope\n",
      "Dataset/SSID_Images//2.jpg: a person on a snowboard on a mountain\n",
      "Dataset/SSID_Images//3.jpg: a man climbing up a snowy mountain\n",
      "Dataset/SSID_Images//4.jpg: a man standing on top of a mountain\n",
      "Dataset/SSID_Images//5.jpg: a man sitting on top of a snowy mountain\n",
      "Dataset/SSID_Images//6.jpg: a man climbing up a mountain with a helmet on\n",
      "Dataset/SSID_Images//7.jpg: a field with a fence and mountains in the background\n",
      "Dataset/SSID_Images//8.jpg: a man with a backpack on a trail\n",
      "Dataset/SSID_Images//9.jpg: the summit of the mountain is covered in snow\n",
      "Dataset/SSID_Images//10.jpg: a man wearing a blue shirt\n"
     ]
    }
   ],
   "source": [
    "for img, caption in image_captions.items():\n",
    "    print(f\"{img}: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organising Image Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset/Organized_Annotations/SSID_Train_Organized.json') as f:\n",
    "    organized_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_data = {}\n",
    "\n",
    "album_limit = 100\n",
    "processed_album_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for album_id in organized_data.items():\n",
    "    if processed_album_count >= album_limit:\n",
    "        break \n",
    "\n",
    "    album_id = album_id[1]\n",
    "\n",
    "    # get the value of the first key in the dictionary\n",
    "    stories = album_id[list(album_id.keys())[0]]\n",
    "\n",
    "    for item in stories:\n",
    "\n",
    "        # add a key value pair to the dictionary, key being the image_id and value being the storytext\n",
    "        fine_tune_data[item['image_id']] = item['storytext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625\n"
     ]
    }
   ],
   "source": [
    "print(len(fine_tune_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_fine_tune_data = dict(sorted(fine_tune_data.items(), key=lambda x: int(x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted_fine_tune_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, data_dict, image_folder, processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dict (dict): A dictionary where keys are image names, and values are captions.\n",
    "            image_folder (str): Path to the folder containing the images.\n",
    "            processor (BlipProcessor): The processor to preprocess the images and captions.\n",
    "        \"\"\"\n",
    "        self.data_dict = data_dict\n",
    "        self.image_folder = image_folder\n",
    "        self.processor = processor\n",
    "        self.image_keys = list(data_dict.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_keys)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_key = self.image_keys[idx]\n",
    "        caption = self.data_dict[image_key]\n",
    "        \n",
    "        image_path = f\"{self.image_folder}/{image_key}.jpg\"  \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        inputs = self.processor(images=image, text=caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs['input_ids'].squeeze(),  \n",
    "            \"attention_mask\": inputs['attention_mask'].squeeze(),  \n",
    "            \"pixel_values\": inputs['pixel_values'].squeeze()  \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_fine_tune_data = dict(list(sorted_fine_tune_data.items())[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dataset/SSID_Images//1.jpg': 'a group of people walking up a snowy slope', 'Dataset/SSID_Images//2.jpg': 'a person on a snowboard on a mountain', 'Dataset/SSID_Images//3.jpg': 'a man climbing up a snowy mountain', 'Dataset/SSID_Images//4.jpg': 'a man standing on top of a mountain', 'Dataset/SSID_Images//5.jpg': 'a man sitting on top of a snowy mountain', 'Dataset/SSID_Images//6.jpg': 'a man climbing up a mountain with a helmet on', 'Dataset/SSID_Images//7.jpg': 'a field with a fence and mountains in the background', 'Dataset/SSID_Images//8.jpg': 'a man with a backpack on a trail', 'Dataset/SSID_Images//9.jpg': 'the summit of the mountain is covered in snow', 'Dataset/SSID_Images//10.jpg': 'a man wearing a blue shirt'}\n"
     ]
    }
   ],
   "source": [
    "print(image_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 'a group of people walking up a snowy slope', '2': 'a person on a snowboard on a mountain', '3': 'a man climbing up a snowy mountain', '4': 'a man standing on top of a mountain', '5': 'a man sitting on top of a snowy mountain', '6': 'a man climbing up a mountain with a helmet on', '7': 'a field with a fence and mountains in the background', '8': 'a man with a backpack on a trail', '9': 'the summit of the mountain is covered in snow', '10': 'a man wearing a blue shirt'}\n"
     ]
    }
   ],
   "source": [
    "image_captions = {k.split('/')[-1].split('.')[0]: v for k, v in image_captions.items()}\n",
    "\n",
    "print(image_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pdogg Windows10\\Desktop\\Semester 7\\Computer Vision\\Reinforcement-Learning-Enhanced-Visual-Storytelling\\cv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "learning_rate = 5e-5\n",
    "batch_size = 5\n",
    "\n",
    "dataset = ImageCaptionDataset(image_captions, image_folder, processor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    for batch in data_loader:\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, pixel_values=pixel_values, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = []\n",
    "image_paths = []\n",
    "\n",
    "for i in range(11, 111):\n",
    "    image_path = f\"{image_folder}/{i}.jpg\"\n",
    "    image_paths.append(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

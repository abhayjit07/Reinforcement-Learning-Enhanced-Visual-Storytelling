{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, CLIPProcessor, CLIPModel, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksubh\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the BLIP model and processor\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate captions\n",
    "def generate_caption_with_blip(image_path):\n",
    "    # Load and process the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate caption using BLIP model\n",
    "    with torch.no_grad():\n",
    "        output = blip_model.generate(**inputs)\n",
    "    caption = blip_processor.decode(output[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksubh\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/SSID_Images//1.jpg: a group of people walking up a snowy slope\n",
      "Dataset/SSID_Images//2.jpg: a person on a snowboard on a mountain\n",
      "Dataset/SSID_Images//3.jpg: a man climbing up a snowy mountain\n",
      "Dataset/SSID_Images//4.jpg: a man standing on top of a mountain\n",
      "Dataset/SSID_Images//5.jpg: a man sitting on top of a snowy mountain\n",
      "Dataset/SSID_Images//6.jpg: a man climbing up a mountain with a helmet on\n",
      "Dataset/SSID_Images//7.jpg: a field with a fence and mountains in the background\n",
      "Dataset/SSID_Images//8.jpg: a man with a backpack on a trail\n",
      "Dataset/SSID_Images//9.jpg: the summit of the mountain is covered in snow\n",
      "Dataset/SSID_Images//10.jpg: a man wearing a blue shirt\n"
     ]
    }
   ],
   "source": [
    "# Test the function on a set of images\n",
    "image_folder = 'Dataset/SSID_Images/'\n",
    "image_paths = [f\"{image_folder}/{i}.jpg\" for i in range(1, 11)]\n",
    "\n",
    "captions = {image_path: generate_caption_with_blip(image_path) for image_path in image_paths}\n",
    "\n",
    "for img, caption in captions.items():\n",
    "    print(f\"{img}: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7140d90bf843288abe62a0c66185dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksubh\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ksubh\\.cache\\huggingface\\hub\\models--nlpconnect--vit-gpt2-image-captioning. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816e9677930c4169a6223644519d2cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/982M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce768ebfd88d41b1aab76d2e96ff4dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksubh\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c3c4f9ab4a4b11ac6d876fa2111e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/241 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7509ee8496464220943546f4217563ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce35d93df47477cbf712ca287fa1bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33611962700a4607907c37fcd4346540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4e6e0444f44e538eb4da8a4768d8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the Vision Encoder Decoder model and necessary processors\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (crossattention): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (q_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate captions\n",
    "def generate_caption_with_vit_gpt2(image_path):\n",
    "    # Load and process the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    # Generate caption using the model\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(pixel_values, max_length=50, num_beams=4, do_sample=False)\n",
    "\n",
    "    # Decode the output tokens to a string\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/SSID_Images//1.jpg: people walking on top of a snow covered slope \n",
      "Dataset/SSID_Images//2.jpg: a mountain range with snow capped mountains \n",
      "Dataset/SSID_Images//3.jpg: a person riding skis on top of a snow covered slope \n",
      "Dataset/SSID_Images//4.jpg: a person standing on top of a snow covered mountain \n",
      "Dataset/SSID_Images//5.jpg: a man sitting in the snow next to a pile of snow \n",
      "Dataset/SSID_Images//6.jpg: a person on a snowboard in the snow \n",
      "Dataset/SSID_Images//7.jpg: a field with a fence and a mountain range \n",
      "Dataset/SSID_Images//8.jpg: a man riding skis down a snow covered slope \n",
      "Dataset/SSID_Images//9.jpg: a mountain range with snow capped mountains \n",
      "Dataset/SSID_Images//10.jpg: a man in a hat talking on a cell phone \n",
      "Dataset/SSID_Images//11.jpg: a woman standing next to a parking meter \n",
      "Dataset/SSID_Images//12.jpg: two people riding skis on top of a dirt road \n",
      "Dataset/SSID_Images//13.jpg: a man on skis standing on top of a snow covered slope \n",
      "Dataset/SSID_Images//14.jpg: a person on skis standing on top of a mountain \n",
      "Dataset/SSID_Images//15.jpg: a mountain range with snow capped mountains \n",
      "Dataset/SSID_Images//16.jpg: a person riding skis down a snow covered slope \n",
      "Dataset/SSID_Images//17.jpg: a mountain range with snow capped mountains \n",
      "Dataset/SSID_Images//18.jpg: a person riding skis down a snow covered slope \n",
      "Dataset/SSID_Images//19.jpg: two people on skis standing on top of a snow covered slope \n",
      "Dataset/SSID_Images//20.jpg: a man holding a camera while standing in the snow \n",
      "Dataset/SSID_Images//21.jpg: a man riding skis on top of a snow covered slope \n",
      "Dataset/SSID_Images//22.jpg: a person on skis jumping over a snow covered slope \n",
      "Dataset/SSID_Images//23.jpg: two people standing on top of a snow covered slope \n",
      "Dataset/SSID_Images//24.jpg: a mountain range with a sky background \n",
      "Dataset/SSID_Images//25.jpg: a man standing on top of a snow covered mountain \n",
      "Dataset/SSID_Images//26.jpg: a person sitting on a bench looking at the sky \n",
      "Dataset/SSID_Images//27.jpg: a man standing on top of a grass covered field \n",
      "Dataset/SSID_Images//28.jpg: a woman is looking at a drawing of a cat on a table \n",
      "Dataset/SSID_Images//29.jpg: a woman standing in front of a wall with a painting on it \n",
      "Dataset/SSID_Images//30.jpg: a book is sitting on top of a book shelf \n",
      "Dataset/SSID_Images//31.jpg: a room filled with lots of clutter and clutter on the floor \n",
      "Dataset/SSID_Images//32.jpg: a man running across a lush green field \n",
      "Dataset/SSID_Images//33.jpg: a man standing on top of a mountain with a backpack \n",
      "Dataset/SSID_Images//34.jpg: a man standing on top of a rocky hillside \n",
      "Dataset/SSID_Images//35.jpg: a man walking down a hill with a backpack \n",
      "Dataset/SSID_Images//36.jpg: three young men sitting on a rock wall \n",
      "Dataset/SSID_Images//37.jpg: two giraffes are standing near a rock wall \n",
      "Dataset/SSID_Images//38.jpg: a person standing on a stone wall with a rock wall \n",
      "Dataset/SSID_Images//39.jpg: a man on a ledge climbing up a wall \n",
      "Dataset/SSID_Images//40.jpg: a person jumping up into the air on a skateboard \n",
      "Dataset/SSID_Images//41.jpg: a man in a hat and a man in a hat standing next to each other \n",
      "Dataset/SSID_Images//42.jpg: a man sitting in a car with a remote in his hand \n",
      "Dataset/SSID_Images//43.jpg: a man sitting on the back of a white truck \n",
      "Dataset/SSID_Images//44.jpg: two men standing next to each other in a parking lot \n",
      "Dataset/SSID_Images//45.jpg: a number of people on a boat in the water \n",
      "Dataset/SSID_Images//46.jpg: people on a boat in the water \n",
      "Dataset/SSID_Images//47.jpg: a man riding a wave on top of a surfboard \n",
      "Dataset/SSID_Images//48.jpg: a man in a hat is standing in the water \n",
      "Dataset/SSID_Images//49.jpg: a man wearing a hat and holding a cell phone \n",
      "Dataset/SSID_Images//50.jpg: a man riding a wave on top of a surfboard \n",
      "Dataset/SSID_Images//51.jpg: a man and a woman sitting on a boat \n",
      "Dataset/SSID_Images//52.jpg: a fighter jet flying through the air \n",
      "Dataset/SSID_Images//53.jpg: a person standing on top of a pile of rocks \n",
      "Dataset/SSID_Images//54.jpg: a person holding a surfboard in a body of water \n",
      "Dataset/SSID_Images//55.jpg: a man in a wet suit with a fish in his mouth \n",
      "Dataset/SSID_Images//56.jpg: a woman in a wetsuit is holding a paddle \n",
      "Dataset/SSID_Images//57.jpg: a person standing in the middle of a large body of water \n",
      "Dataset/SSID_Images//58.jpg: a person standing on top of a pile of rocks \n",
      "Dataset/SSID_Images//59.jpg: a person standing next to a bunch of birds \n",
      "Dataset/SSID_Images//60.jpg: a man holding a surfboard next to a woman \n",
      "Dataset/SSID_Images//61.jpg: a woman standing on a surfboard in the ocean \n",
      "Dataset/SSID_Images//62.jpg: a person cutting a piece of bread on a wooden board \n",
      "Dataset/SSID_Images//63.jpg: a bird eating a hot dog on a grill \n",
      "Dataset/SSID_Images//64.jpg: a person cutting a steak with a knife \n",
      "Dataset/SSID_Images//65.jpg: a man standing next to a woman on a beach \n",
      "Dataset/SSID_Images//66.jpg: a man standing on top of a lush green field \n",
      "Dataset/SSID_Images//67.jpg: a man holding a frisbee next to a woman \n",
      "Dataset/SSID_Images//68.jpg: a person throwing a frisbee in a field \n",
      "Dataset/SSID_Images//69.jpg: a person laying in the grass with a cell phone \n",
      "Dataset/SSID_Images//70.jpg: a man holding a paper plate with a frisbee in his hand \n",
      "Dataset/SSID_Images//71.jpg: a woman sitting in front of a table with bananas \n",
      "Dataset/SSID_Images//72.jpg: a woman walking a horse down a dirt road \n",
      "Dataset/SSID_Images//73.jpg: a man in a hat talking on a cell phone \n",
      "Dataset/SSID_Images//74.jpg: two men are cutting a cake together \n",
      "Dataset/SSID_Images//75.jpg: a mountain range with a sky background \n",
      "Dataset/SSID_Images//76.jpg: people walking through a forest \n",
      "Dataset/SSID_Images//77.jpg: a black and white bird flying through the air \n",
      "Dataset/SSID_Images//78.jpg: a man standing next to a woman holding a frisbee \n",
      "Dataset/SSID_Images//79.jpg: an aerial view of a forest filled with trees \n",
      "Dataset/SSID_Images//80.jpg: a man walking through a forest filled with trees \n",
      "Dataset/SSID_Images//81.jpg: a scenic view of a river with mountains \n",
      "Dataset/SSID_Images//82.jpg: two people are paddling a canoe through the water \n",
      "Dataset/SSID_Images//83.jpg: a man and a woman in a canoe on a river \n",
      "Dataset/SSID_Images//84.jpg: two people on a paddle board in the water \n",
      "Dataset/SSID_Images//85.jpg: a man holding a stick in front of a body of water \n",
      "Dataset/SSID_Images//86.jpg: a man standing in front of a television screen \n",
      "Dataset/SSID_Images//87.jpg: a man is looking at his cell phone \n",
      "Dataset/SSID_Images//88.jpg: a man standing in front of a group of people \n",
      "Dataset/SSID_Images//89.jpg: a man in a black shirt is looking at something \n",
      "Dataset/SSID_Images//90.jpg: a man with a beard standing in front of a green screen \n",
      "Dataset/SSID_Images//91.jpg: a man standing on top of a rock near a body of water \n",
      "Dataset/SSID_Images//92.jpg: a rocky cliff overlooking a body of water \n",
      "Dataset/SSID_Images//93.jpg: a man standing on a rock with a camera \n",
      "Dataset/SSID_Images//94.jpg: a man with a beard standing next to a rock \n",
      "Dataset/SSID_Images//95.jpg: a man standing on top of a rock next to a river \n",
      "Dataset/SSID_Images//96.jpg: people on a boat in the water \n",
      "Dataset/SSID_Images//97.jpg: a body of water with trees and a river \n",
      "Dataset/SSID_Images//98.jpg: a small boat in the middle of the ocean \n",
      "Dataset/SSID_Images//99.jpg: a bird standing on top of a rock \n"
     ]
    }
   ],
   "source": [
    "# Test the function on a set of images\n",
    "image_folder = 'Dataset/SSID_Images/'\n",
    "image_paths = [f\"{image_folder}/{i}.jpg\" for i in range(1, 100)]\n",
    "\n",
    "captions = {image_path: generate_caption_with_vit_gpt2(image_path) for image_path in image_paths}\n",
    "\n",
    "for img, caption in captions.items():\n",
    "    print(f\"{img}: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final concepts: ['a man standing on top of a mountain', 'a group of people walking up a snowy slope', 'a person on a snowboard on a mountain', 'a man sitting on top of a snowy mountain', 'a man climbing up a snowy mountain']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# Load the BLIP model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Define the image paths\n",
    "image_paths = [\n",
    "    'Dataset/SSID_Images//1.jpg',\n",
    "    'Dataset/SSID_Images//2.jpg',\n",
    "    'Dataset/SSID_Images//3.jpg',\n",
    "    'Dataset/SSID_Images//4.jpg',\n",
    "    'Dataset/SSID_Images//5.jpg'\n",
    "]\n",
    "\n",
    "# Function to generate a caption (concept) for each image\n",
    "def generate_concept(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate a caption for the image\n",
    "    caption_ids = model.generate(**inputs)\n",
    "    caption = processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Generate concepts for each image and store them\n",
    "all_concepts = []\n",
    "for path in image_paths:\n",
    "    concept = generate_concept(path)\n",
    "    all_concepts.append(concept)\n",
    "\n",
    "# Deduplicate the concepts if needed\n",
    "def remove_noise(concepts):\n",
    "    return list(set(concepts))  # Deduplicate concepts\n",
    "\n",
    "# Get the final concepts for all images\n",
    "final_concepts = remove_noise(all_concepts)\n",
    "print(\"Final concepts:\", final_concepts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image to feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksubh\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ksubh\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall feature vector: tensor([[-2.6171e-01,  1.1769e-01, -5.0845e-02,  1.1002e-01, -2.2947e-01,\n",
      "         -8.2764e-02, -8.0033e-02, -3.9292e-01, -5.5811e-02,  1.0190e-01,\n",
      "          1.3146e-01, -7.7623e-02,  4.3106e-01,  2.9742e-02,  1.7267e-01,\n",
      "          3.0587e-02, -1.0321e-01,  3.0394e-01, -1.0508e-01,  6.1711e-02,\n",
      "         -1.0026e-01,  3.1109e-01,  5.8501e-02, -4.9589e-01,  2.5234e-02,\n",
      "          3.2454e-01,  7.1826e-02,  1.4119e-02, -9.1528e-02, -1.3554e-01,\n",
      "          2.4361e-01,  9.9473e-04, -1.6967e-02, -2.2359e-01, -1.2834e-01,\n",
      "         -7.6284e-02,  1.0232e-01,  1.9060e-01, -1.1656e-01,  8.8653e-03,\n",
      "          9.0522e-05, -1.0759e-01, -3.0686e-01,  2.3899e-01,  3.8309e-01,\n",
      "          1.5262e-01, -2.4723e-01, -1.4980e-01, -2.0201e-02, -3.8295e-02,\n",
      "          4.3377e-01, -7.6373e-02, -7.5963e-02,  2.3580e-01, -2.0909e-02,\n",
      "         -3.5132e-01,  1.3786e-01,  2.5395e-01,  3.5936e-02, -1.3418e-01,\n",
      "          1.4532e-01, -1.2905e-01,  7.3284e-03,  1.2222e-01,  1.0340e-01,\n",
      "         -2.9831e-01, -8.1295e-02,  2.0981e-01, -4.6872e-03, -1.3925e-01,\n",
      "         -5.3036e-01, -1.9656e-01,  1.4102e-01, -1.0119e-02, -4.9501e-02,\n",
      "          1.3696e-01,  3.0009e-01,  1.5775e-01,  1.1540e-02, -2.2174e-01,\n",
      "         -2.7609e-01,  1.3449e-01,  2.9726e-01,  2.6927e-01, -9.7626e-02,\n",
      "          9.1000e-02, -2.0248e-01, -1.0674e-01, -2.4824e-01,  6.9828e-02,\n",
      "         -1.6089e-01,  1.3448e-02, -8.4471e-01, -4.8551e-02,  1.4604e-02,\n",
      "         -1.4739e-01, -1.6605e-01,  1.1803e-01,  3.4977e-01, -5.1265e-01,\n",
      "          5.2996e-01,  3.9163e-01,  6.4680e-02,  2.6015e-01, -1.6353e-01,\n",
      "         -1.6090e-02, -3.8260e-01,  1.7755e-01, -2.3972e-01, -1.3972e-01,\n",
      "         -1.1826e-01,  1.0301e-01,  4.1936e-02,  1.0602e-01, -1.1592e-01,\n",
      "          2.6365e-01, -2.3215e-01,  3.6295e-01, -2.1239e-01, -1.3035e-01,\n",
      "         -1.1423e-02, -3.4007e-01, -2.1248e-01, -2.8834e-01, -1.8580e-02,\n",
      "          4.9603e-02,  4.8892e-02, -1.8949e-01, -2.8308e-01,  1.4100e-01,\n",
      "          2.2532e-01, -4.9033e-02, -4.0664e-01,  3.6706e+00, -4.6535e-01,\n",
      "         -1.8194e-01, -5.8456e-02, -1.1484e-01, -2.8252e-01,  2.7133e-02,\n",
      "         -2.1362e-01, -2.3648e-01, -2.5950e-01,  4.5896e-01, -3.7895e-01,\n",
      "         -1.8130e-01,  1.2717e-01,  1.3745e-01,  2.0372e-01, -7.3271e-02,\n",
      "         -3.0669e-01, -1.1769e-02,  3.9451e-02,  2.0266e-01,  1.8162e-01,\n",
      "         -3.1999e-01, -5.7815e-02, -2.1120e-01,  6.8314e-02,  2.8215e-01,\n",
      "         -2.0643e-02,  7.2393e-02,  6.4835e-02,  1.8720e-01, -1.6007e-01,\n",
      "          2.7025e-01,  2.9335e-01, -1.0497e-01,  1.9950e-01,  2.6022e-02,\n",
      "          1.7918e-01, -1.6245e-02,  4.2634e-02,  7.0422e-02, -1.0621e-01,\n",
      "          3.8844e-01, -5.9186e-02, -3.3195e-02, -1.7018e-01, -3.8524e-02,\n",
      "         -2.2812e-01,  3.4093e-01, -2.8866e-01, -7.2925e-02, -8.0711e-02,\n",
      "         -2.2208e-01,  1.1381e-01, -3.3002e-01,  1.9898e-01,  3.1039e-01,\n",
      "          3.3850e-02,  1.5065e-01, -1.3400e-01,  2.1865e-01, -1.8739e-01,\n",
      "          1.7229e-01, -2.0646e-01,  3.7892e-01,  1.7434e-01, -8.0009e-02,\n",
      "          2.0508e-01, -3.3685e-02, -6.9787e-02,  8.9719e-03,  5.6708e-02,\n",
      "         -4.2887e-01,  2.8120e-01, -1.6383e-01, -1.8924e-01,  1.4696e-01,\n",
      "         -7.4515e-02,  4.0241e-01, -7.1436e-02,  3.4752e-01,  7.1493e-02,\n",
      "         -8.1502e-02, -1.2131e-01,  6.4669e-02,  2.2511e-01, -9.6789e-02,\n",
      "          2.5495e-02, -3.8743e-02, -1.2313e-01, -1.9566e-01, -2.7020e-01,\n",
      "          1.9586e-02, -4.1963e-02, -3.7830e-01, -8.7710e-02, -1.0765e-01,\n",
      "         -1.1764e-01, -6.6101e-02, -4.3953e-02,  2.0572e-01,  2.2627e-01,\n",
      "          3.9710e-01,  5.5303e-01, -3.1094e-01, -1.5640e-01, -2.4541e-01,\n",
      "          3.4626e-01,  3.8732e-01, -1.0180e-01,  5.5942e-02, -1.3869e-01,\n",
      "         -3.8246e-01, -6.4742e-02,  7.1316e-02, -1.1708e-02, -1.1663e-01,\n",
      "         -1.9014e-01, -3.3623e-02,  3.5637e-01,  1.7225e-01,  2.4626e-02,\n",
      "         -1.1657e-01,  1.3467e-01, -3.7294e-01, -1.7911e-01,  1.2598e-01,\n",
      "          3.5835e-01,  5.1264e-02,  1.3806e-01,  1.6344e-02, -1.3958e-02,\n",
      "         -1.8579e-01,  1.0634e-01, -2.0340e-01, -1.1074e-03,  1.3018e-01,\n",
      "          8.7327e-02,  3.4866e-02,  5.3039e-01, -9.8704e-02, -1.0159e-01,\n",
      "         -5.1576e-03,  3.0186e-01, -1.4402e-01,  2.2073e-01, -2.9408e-01,\n",
      "         -4.3871e-02, -3.6900e-02,  7.7841e-01,  1.9445e-01, -1.8398e-01,\n",
      "          2.8686e-01, -1.0485e-01, -1.2728e-01, -8.5857e-02, -4.9404e-01,\n",
      "         -1.1688e-01, -9.4285e-02, -1.8047e-01,  7.5761e-02, -3.4733e-01,\n",
      "          3.7830e-01, -1.9695e-01, -1.8595e-02, -3.1718e-01, -1.7862e-01,\n",
      "         -1.8333e-01,  7.0979e-02,  3.8400e-01,  8.2960e-02, -2.6487e-01,\n",
      "         -9.3576e-01,  3.3251e-01, -2.8678e-02,  3.5207e-01, -1.3311e-01,\n",
      "          1.3088e-01,  3.2438e-01,  3.6660e+00,  1.6123e-01, -1.5303e-01,\n",
      "          2.2927e-01, -1.8163e-01, -3.6018e-01, -5.4085e-02, -3.4411e-01,\n",
      "          4.6551e-02,  2.5807e-01,  5.3618e-02,  8.3327e-03,  1.8186e-01,\n",
      "          1.8361e-01,  1.2595e-01, -9.0685e-03,  1.4137e-01, -9.8873e-01,\n",
      "         -7.9190e-02, -1.4820e-01,  2.0754e-01, -6.5369e-02, -1.5746e-01,\n",
      "         -5.3408e-02, -1.4641e-01,  1.4204e-01,  3.0377e-01,  1.7915e-01,\n",
      "         -4.1199e-02, -4.4170e-03, -1.2488e-02, -2.3878e-01,  1.1791e-01,\n",
      "         -2.7331e-01,  3.1909e-01, -1.1793e-01,  9.0530e-02,  2.2787e-02,\n",
      "          2.7522e-01,  7.7530e-02,  2.1857e-01, -1.7349e-01, -1.1796e-01,\n",
      "          8.6960e-02,  1.2129e-02, -2.1676e-01,  3.2650e-02,  2.2985e-01,\n",
      "          7.1857e-01, -5.0230e-01, -4.3012e-01,  2.0612e-01,  1.8929e-01,\n",
      "         -2.3113e-01, -5.9799e-02,  2.1088e-01, -1.1647e-01,  3.2760e-01,\n",
      "         -9.4976e-02, -4.0092e-02,  3.2617e-01,  1.2549e-01, -1.3644e-01,\n",
      "          4.5239e-02, -2.4158e-01, -3.9462e-01,  6.6943e-02, -2.3255e-01,\n",
      "          9.5276e-02,  7.1834e-02, -1.1224e-01, -2.2167e-01,  3.0476e-01,\n",
      "         -4.9863e-02, -1.0967e-01, -2.0542e-01, -3.2063e-01,  2.1478e-02,\n",
      "         -6.4663e-01,  1.5520e-01, -7.3913e-02, -3.9353e-02,  2.1373e-01,\n",
      "          1.7903e-01,  1.7459e-01,  6.6415e-02, -3.3061e-01,  4.1001e-01,\n",
      "          5.6764e-02, -1.9394e-01,  4.1940e-01,  3.9693e-02, -2.0713e-01,\n",
      "         -6.3247e-03,  2.9938e-01, -3.1259e-01,  2.2698e-01,  2.6205e-01,\n",
      "          5.3528e-01,  2.5765e-01,  9.1899e-02,  4.2486e-01, -5.9418e-02,\n",
      "          2.3434e-01, -2.1237e-01, -1.2586e-01,  1.3273e-01,  3.3847e-02,\n",
      "          4.6515e-01,  2.1183e-01, -2.2124e-01, -6.1541e-01,  4.0967e-01,\n",
      "         -7.4572e-02,  3.2342e-02,  1.7058e-01,  1.1817e-01,  2.5235e-01,\n",
      "         -1.8316e-01,  3.8145e-02, -1.3608e-01,  3.5768e-02, -3.1606e-01,\n",
      "         -2.0864e-01, -4.5733e-02,  1.7016e-01, -2.2531e-01, -1.9463e-01,\n",
      "         -1.7179e-01,  2.1570e-01, -1.6293e-01, -1.2200e-01, -8.4057e-02,\n",
      "          1.1806e-01, -3.5520e-01, -4.2382e-01, -3.6230e-02,  2.3497e-01,\n",
      "         -4.3946e-01,  1.4817e-01, -1.4808e-01,  1.9591e-01,  1.2286e-01,\n",
      "          5.3200e-02, -7.2125e-02, -1.0395e-01, -2.1806e-01, -4.0419e-02,\n",
      "          6.5119e-02, -1.4506e-01,  3.2330e-02,  3.4008e-01,  2.7497e-01,\n",
      "         -1.6880e-01,  5.5504e-02, -3.5158e-01, -5.4929e-02,  2.6894e-01,\n",
      "         -5.6691e-03, -1.1528e-01,  1.5206e-01, -6.4285e-02,  2.5039e-01,\n",
      "         -3.7994e-01,  3.5878e-01,  6.7157e-02,  1.5734e-01, -1.4207e-02,\n",
      "         -2.2056e-01, -1.7683e-01,  2.9694e-01, -5.8146e-02, -3.4458e-01,\n",
      "         -5.4165e-02,  1.6929e-01,  1.4986e-01, -2.8525e-02,  3.2882e-02,\n",
      "          4.7069e-01,  3.2896e-02, -3.4193e-01,  7.9754e-01, -4.5747e-03,\n",
      "         -1.5664e-01, -6.7798e-03, -2.0610e-01, -5.4400e-01,  1.4509e-01,\n",
      "         -4.5456e-02,  1.0523e-01, -1.5870e-01,  3.8418e-01,  2.9532e-01,\n",
      "         -2.4451e-01, -5.3474e-01,  3.9058e-01,  5.4807e-02,  7.7456e-02,\n",
      "         -2.7341e-02, -5.6764e-01]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration, CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load the BLIP model for caption generation\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load the CLIP model for feature extraction\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def generate_overall_feature_vector(image_paths):\n",
    "    \"\"\"\n",
    "    Generates an overall feature vector for a list of images by averaging the CLIP feature vectors\n",
    "    of BLIP-generated captions for each image.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of image file paths.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Averaged feature vector representing the input images.\n",
    "    \"\"\"\n",
    "    # Function to generate a caption (concept) for each image\n",
    "    def generate_concept(image_path):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = blip_processor(images=image, return_tensors=\"pt\")\n",
    "        caption_ids = blip_model.generate(**inputs)\n",
    "        caption = blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "\n",
    "    # Function to encode captions into feature vectors using CLIP\n",
    "    def encode_concept(concept):\n",
    "        text_inputs = clip_processor(text=[concept], return_tensors=\"pt\", padding=True)\n",
    "        text_features = clip_model.get_text_features(**text_inputs)\n",
    "        return text_features\n",
    "\n",
    "    # Generate concepts for each image and encode them\n",
    "    concept_vectors = []\n",
    "    for path in image_paths:\n",
    "        concept = generate_concept(path)\n",
    "        vector = encode_concept(concept)\n",
    "        concept_vectors.append(vector)\n",
    "\n",
    "    # Calculate the final feature vector by averaging\n",
    "    overall_feature_vector = torch.mean(torch.stack(concept_vectors), dim=0)\n",
    "    \n",
    "    return overall_feature_vector\n",
    "\n",
    "# Example usage\n",
    "image_paths = [\n",
    "    'Dataset/SSID_Images//1.jpg',\n",
    "    'Dataset/SSID_Images//2.jpg',\n",
    "    'Dataset/SSID_Images//3.jpg',\n",
    "    'Dataset/SSID_Images//4.jpg',\n",
    "    'Dataset/SSID_Images//5.jpg'\n",
    "]\n",
    "\n",
    "# Call the function and get the overall feature vector\n",
    "overall_feature_vector_image = generate_overall_feature_vector(image_paths)\n",
    "print(\"Overall feature vector:\", overall_feature_vector_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story to feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksubh\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall feature vector for the story text based on RAKE keywords: tensor([[-5.4812e-02, -2.7397e-02,  1.1542e-02,  1.0613e-01, -8.0487e-02,\n",
      "         -1.7179e-01, -9.4563e-02, -1.2755e+00, -2.9355e-02,  2.1567e-01,\n",
      "          2.7565e-02, -3.3601e-02, -8.7047e-02, -7.9168e-02,  1.7152e-01,\n",
      "          3.9473e-02,  2.7349e-01,  6.0431e-02, -9.6591e-02, -1.4670e-01,\n",
      "          2.7230e-01,  3.9014e-02,  6.8657e-02, -6.6495e-02, -1.2100e-01,\n",
      "          2.5792e-02, -2.6740e-02,  2.0362e-02, -4.2674e-02,  8.4710e-02,\n",
      "         -2.3487e-02, -1.4950e-01,  6.3557e-03, -5.1198e-03, -3.2215e-02,\n",
      "          2.4282e-01,  6.3096e-02,  8.8220e-03, -4.3158e-02, -2.0463e-02,\n",
      "         -5.5950e-02, -9.6385e-02, -6.9654e-02,  1.0052e-01,  9.4145e-02,\n",
      "          3.0459e-01, -6.6751e-02,  7.0225e-02,  1.0621e-01,  3.9043e-02,\n",
      "         -1.6095e-02, -6.2083e-02,  1.0096e-01, -1.5872e-01,  3.8540e-02,\n",
      "          7.2946e-03, -2.3128e-02,  8.1000e-03, -6.9978e-02, -3.0512e-02,\n",
      "          2.2941e-01, -6.3825e-02,  1.3822e-01, -1.2597e-01, -6.9041e-03,\n",
      "         -2.0249e-01, -6.4380e-02, -1.5510e-01, -1.9563e-02, -5.1003e-02,\n",
      "          1.6452e-02, -9.5233e-02, -9.7758e-03,  8.7056e-02, -5.6340e-02,\n",
      "         -1.2384e-01,  1.2050e-01,  6.6762e-02, -5.6755e-02, -2.1801e-01,\n",
      "         -1.6600e-01, -2.5920e-02, -1.3762e-02,  2.6921e-01,  2.4829e-03,\n",
      "          2.5764e-02,  1.2070e-02, -1.2422e-01,  1.1461e-01, -2.6921e-02,\n",
      "          2.6460e-02, -2.7538e-03, -1.7541e+00,  2.7502e-01, -2.8442e-02,\n",
      "          6.4481e-02,  6.3997e-02,  9.8023e-02,  1.7927e-02, -6.1886e-02,\n",
      "          1.7854e-01,  9.2043e-02,  2.2896e-01, -2.9215e-02, -2.3943e-01,\n",
      "         -4.8567e-02,  1.0189e-01, -2.8924e-02, -1.0962e-01,  8.8865e-04,\n",
      "          3.4735e-02,  2.2205e-01, -9.9106e-02,  3.4449e-02, -2.0890e-03,\n",
      "         -1.2494e-01,  5.5548e-03,  1.9476e-01, -8.6790e-02,  4.5327e-02,\n",
      "          1.3183e-01, -1.7833e-01, -1.4278e-02,  4.7551e-02,  9.3522e-02,\n",
      "         -1.0703e-01, -5.8045e-02, -5.4460e-02, -6.2101e-03,  2.4066e-01,\n",
      "          1.1681e-01,  1.6989e-01, -1.1297e-01,  6.9818e+00,  2.2921e-02,\n",
      "          9.6831e-02, -8.3451e-02, -2.4188e-01, -6.0774e-02,  5.5228e-02,\n",
      "          6.8576e-03, -6.3484e-02, -1.0680e-01,  1.1436e-01, -2.4505e-01,\n",
      "         -1.6759e-02,  4.5661e-02,  3.9052e-02, -1.3445e-01, -2.5736e-02,\n",
      "          1.1100e-01, -1.4235e-01, -1.1439e-02,  7.8571e-02, -1.5234e-02,\n",
      "         -1.0646e-01,  1.7483e-02,  4.6441e-02, -1.3128e-01,  2.8357e-02,\n",
      "         -1.0389e-01, -1.3973e-02, -8.5246e-02, -3.5260e-02,  1.4614e-02,\n",
      "          4.3908e-02,  2.0925e-02, -1.0436e-01,  1.3852e-01, -1.8848e-02,\n",
      "          1.0690e-01,  5.3753e-02, -4.8378e-02,  3.2233e-02, -2.0569e-01,\n",
      "          6.0902e-02, -2.0784e-01, -1.3106e-02,  3.6617e-02,  4.5655e-02,\n",
      "          1.9015e-01,  8.7351e-02, -7.5125e-03, -5.1744e-02, -1.3474e-01,\n",
      "         -2.9411e-02,  1.1981e-01, -1.5587e-01,  7.1361e-02, -5.5983e-02,\n",
      "          4.8452e-02, -1.4344e-01,  2.0804e-03,  8.0275e-02, -2.2061e-03,\n",
      "          8.9318e-02, -4.6496e-03,  1.3651e-01, -1.9298e-02, -7.8151e-02,\n",
      "          7.0824e-02, -6.7208e-02, -7.5569e-02,  1.2709e-01,  3.2593e-02,\n",
      "         -7.5366e-02, -4.5700e-02, -8.4406e-02, -8.6818e-03,  4.1858e-02,\n",
      "          3.4456e-01,  1.9101e-01,  2.4901e-02,  3.2058e-02,  6.0277e-03,\n",
      "         -4.9443e-02, -3.3765e-02, -1.1269e-01, -4.6122e-02, -1.2456e-01,\n",
      "         -1.5539e-01, -1.5079e-01, -7.1435e-02, -6.4213e-02,  1.0016e-02,\n",
      "         -3.6370e-02,  8.5815e-02,  9.8514e-02, -9.7723e-02, -6.2421e-02,\n",
      "          7.2349e-02,  1.0395e-01,  6.2746e-02, -1.5384e-01,  1.2102e-01,\n",
      "          1.8386e-01,  1.1639e-01,  2.3817e-02, -1.1546e-01, -9.0215e-02,\n",
      "          1.0300e-01,  7.2854e-02,  6.1810e-02,  1.6582e-02, -9.0812e-02,\n",
      "         -1.3585e-02,  4.4304e-03, -1.2369e-02, -5.4129e-02, -2.4933e-02,\n",
      "         -1.1386e-01,  1.4425e-01,  1.2490e-01,  1.3327e-01, -8.2664e-02,\n",
      "          8.9800e-02,  3.4987e-02, -1.5627e-01,  8.4615e-02,  6.1474e-02,\n",
      "          6.7941e-02, -8.9176e-02, -2.1343e-03, -6.2688e-02, -3.2732e-02,\n",
      "         -1.1203e-01,  4.6015e-02, -1.1813e-01, -3.6866e-02, -4.9202e-02,\n",
      "          2.4434e-02, -8.0315e-02,  7.3591e-02, -9.7605e-02,  4.3006e-02,\n",
      "         -8.3531e-02,  1.3929e-02,  2.9482e-02,  1.8330e-02, -8.9223e-02,\n",
      "          1.2824e-01, -3.8646e-02,  1.4673e-01,  3.9049e-02,  1.7356e-02,\n",
      "          1.3788e-01,  1.6091e-01, -4.1426e-02,  3.3994e-02, -7.9146e-02,\n",
      "          1.0134e-01, -3.0283e-02,  1.4400e-01,  1.0613e-01, -7.4990e-02,\n",
      "         -1.0743e-01, -2.7771e-01, -1.0196e-01,  2.3643e-02, -5.0733e-03,\n",
      "          5.4376e-02, -2.2224e-03, -5.0954e-02,  6.7708e-03, -1.1062e-02,\n",
      "          8.4424e-02, -1.6483e-02, -1.2888e-03,  6.0946e-02,  6.7622e-02,\n",
      "          5.4664e-02,  4.5058e-02,  6.9739e+00,  1.2149e-01, -5.9760e-02,\n",
      "          1.0042e-01, -9.8687e-02,  3.0665e-02,  7.0865e-02,  1.3009e-01,\n",
      "          1.9504e-01,  4.2702e-01,  8.7487e-02,  2.0166e-02,  9.0255e-02,\n",
      "          3.1528e-02, -3.4032e-02, -2.1942e-02,  8.4540e-02, -2.7141e+00,\n",
      "          2.6445e-02,  9.8486e-02, -1.8642e-03, -1.1854e-02,  1.5067e-02,\n",
      "         -1.3238e-01,  6.3107e-02,  1.7177e-02,  4.0856e-02, -7.6719e-02,\n",
      "         -5.4311e-03, -1.0761e-02, -2.6246e-02, -3.3820e-02, -1.0322e-01,\n",
      "         -3.7296e-02,  8.2880e-02,  1.6608e-01,  2.2274e-02,  3.0341e-02,\n",
      "          8.7115e-02, -2.8913e-02, -1.5102e-02,  1.0128e-02, -1.7933e-02,\n",
      "          3.1089e-01, -3.4997e-02, -3.2145e-03,  2.0964e-02,  3.2799e-02,\n",
      "          6.1600e-02, -8.8552e-02,  9.0083e-02,  2.1701e-01,  7.2308e-02,\n",
      "         -2.7711e-01,  1.0902e-01,  1.1968e-01, -3.1214e-02,  7.6730e-02,\n",
      "         -2.5651e-02, -9.4392e-02, -1.6172e-02,  1.4482e-01, -1.0175e-01,\n",
      "          4.2442e-02, -1.0417e-01,  2.2678e-02, -2.9438e-01,  3.9978e-02,\n",
      "         -1.4486e-01,  5.8888e-02,  5.2485e-03,  1.3762e-02,  2.8542e-03,\n",
      "         -7.6436e-02, -8.9912e-02,  4.5645e-02, -7.8945e-02,  2.4234e-02,\n",
      "         -1.8219e-01, -5.6874e-02, -3.9295e-02, -3.5087e-02,  2.1259e-02,\n",
      "          1.8573e-01,  1.1010e-01, -8.7439e-02, -1.4108e-01,  4.2491e-01,\n",
      "         -1.5802e-02,  3.9166e-02,  6.8746e-02, -2.3566e-02, -1.5255e-01,\n",
      "         -6.1313e-03,  1.1487e-01, -8.7294e-02,  5.5803e-02, -9.0789e-02,\n",
      "          1.8503e-01, -1.8431e-02,  8.9213e-02,  1.5628e-01, -2.6103e-02,\n",
      "          7.7878e-02, -3.6723e-02, -9.5775e-02, -1.3197e-02,  8.7717e-02,\n",
      "          9.5032e-02, -5.3333e-02, -5.6875e-02, -5.2511e-02,  3.4331e-02,\n",
      "          4.2713e-02, -1.2803e-01, -9.8192e-02,  7.0424e-02,  1.6743e-01,\n",
      "         -3.0213e-02, -2.0336e-02, -1.1512e-01,  3.4212e-02,  4.8960e-02,\n",
      "         -1.7273e-02, -6.7476e-03, -7.1297e-03, -2.0940e-01, -2.5966e-02,\n",
      "         -8.1079e-02, -8.1666e-02,  3.9249e-02, -3.8899e-02,  7.6856e-02,\n",
      "          2.8231e-02, -1.2316e-01, -1.8481e-01,  6.9054e-02,  1.6748e-01,\n",
      "          1.6843e-02,  1.9268e-02,  1.2485e-02,  7.4337e-02,  6.0774e-02,\n",
      "          1.1368e-01,  1.0785e-01,  9.3513e-02, -6.3270e-02, -1.4105e-01,\n",
      "          7.6268e-02, -3.4183e-02,  6.2277e-02,  1.2993e-01,  1.5386e-01,\n",
      "          3.6004e-02,  7.9531e-02, -7.7105e-02, -9.2671e-02, -2.6504e-03,\n",
      "          4.4878e-02, -8.4933e-02,  7.9046e-02, -1.9016e-01,  4.4380e-02,\n",
      "          8.0386e-02,  6.2125e-02,  1.6445e-01,  5.8820e-02,  8.3410e-02,\n",
      "          7.6235e-02, -6.5633e-01,  1.0894e-01,  8.9873e-04, -6.7584e-02,\n",
      "         -7.9955e-02,  9.8091e-02,  6.6363e-02, -2.6772e-02,  1.8871e-04,\n",
      "          7.1167e-02,  5.2297e-02, -4.5423e-02,  4.6674e-01, -1.6861e-02,\n",
      "         -1.7008e-02,  1.1594e-01, -5.8162e-02, -1.1638e-01,  6.5330e-02,\n",
      "         -1.2046e-01, -1.4934e-01,  8.1296e-02,  1.2229e-01,  6.1535e-02,\n",
      "         -1.5994e-02, -3.9705e-02, -3.9671e-02, -1.1563e-01, -3.3808e-01,\n",
      "         -5.7847e-02,  2.3937e-03]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from rake_nltk import Rake\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def get_story_feature_vector(story_text):\n",
    "    \"\"\"\n",
    "    Generates an overall feature vector for a story text by averaging the CLIP feature vectors\n",
    "    of keywords extracted using RAKE.\n",
    "\n",
    "    Args:\n",
    "        story_text (str): The story text.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Averaged feature vector representing the story text.\n",
    "    \"\"\"\n",
    "    # Initialize RAKE for keyword extraction\n",
    "    rake = Rake()\n",
    "    rake.extract_keywords_from_text(story_text)\n",
    "    keywords = rake.get_ranked_phrases()  # Get the keywords extracted by RAKE\n",
    "\n",
    "    # Function to encode a keyword into a feature vector using CLIP\n",
    "    def encode_keyword(keyword):\n",
    "        text_inputs = clip_processor(text=[keyword], return_tensors=\"pt\", padding=True)\n",
    "        text_features = clip_model.get_text_features(**text_inputs)\n",
    "        return text_features\n",
    "\n",
    "    # Encode each keyword and store the vectors\n",
    "    keyword_vectors = []\n",
    "    for keyword in keywords:\n",
    "        vector = encode_keyword(keyword)\n",
    "        keyword_vectors.append(vector)\n",
    "\n",
    "    # Calculate the final feature vector by averaging the keyword vectors\n",
    "    overall_feature_vector = torch.mean(torch.stack(keyword_vectors), dim=0)\n",
    "    \n",
    "    return overall_feature_vector\n",
    "\n",
    "# Example usage\n",
    "story_text = \"\"\"\n",
    "Once upon a time, in the heart of the mountains, the group was stopped by a man. \"He's a member of this group. He's called a soldier of an army. You can tell he's an enemy of mine. So, why don't you get a look at him?\" he stared at the snowman, who had no idea where he was. The man had a strange look on his face, and he seemed to be a bit of a weirdo. However, he didn't say anything to the other snow he climbed up to a mountain, his right hand being on the mountain's top. A snowflake-shaped, red-colored mountain that was very steep, like a cave. It was about three meters tall, with a peak that could be reached he looked at his snow-covered face. There was a faint smile on its face as it looked down at a white-haired man, wearing a snow mask. After a moment, it began to grow a little taller. His face was dark red the man's face grew even more, making it seem as though he had been a very powerful and powerful person. As he walked, a small voice spoke from behind him, \"I heard you're coming. Why are you here?\"\n",
    "\"\"\"\n",
    "\n",
    "# Call the function and get the overall feature vector\n",
    "overall_feature_vector_story = get_story_feature_vector(story_text)\n",
    "print(\"Overall feature vector for the story text based on RAKE keywords:\", overall_feature_vector_story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground truths to feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall feature vector for the ground truths: tensor([[-2.0492e-01, -4.7512e-02,  1.5831e-01,  1.0831e-01, -9.8873e-02,\n",
      "         -2.2198e-01, -6.6798e-02, -6.0664e-01, -3.7203e-02,  1.7869e-01,\n",
      "          2.3644e-01, -1.1605e-01,  1.6149e-01, -8.7494e-02,  1.5276e-01,\n",
      "          8.5210e-02, -2.7141e-02,  1.9447e-01, -1.0456e-01,  2.0103e-01,\n",
      "         -2.5094e-01,  4.0982e-04,  1.1917e-02, -7.1495e-01, -1.1327e-01,\n",
      "          2.9432e-01,  1.7969e-02, -8.1345e-02, -1.3319e-02, -6.7199e-02,\n",
      "          2.0573e-01, -2.7942e-02, -9.4614e-03, -2.2664e-01, -2.3932e-01,\n",
      "         -6.3323e-02,  1.3956e-01,  8.6716e-02, -9.1183e-02,  1.1044e-01,\n",
      "         -3.2266e-03, -1.2013e-01, -2.7474e-01,  2.5388e-01,  1.6143e-01,\n",
      "         -9.4454e-02, -1.3122e-01, -1.6551e-01, -7.3807e-02, -2.6146e-01,\n",
      "          2.7608e-01, -1.5043e-01,  2.3500e-02,  1.5725e-01,  2.1333e-01,\n",
      "         -7.3675e-02,  6.6722e-03, -1.1314e-01, -1.3811e-01, -2.4468e-01,\n",
      "          8.8673e-02, -2.3037e-01, -6.2919e-03,  1.0825e-01,  1.1596e-01,\n",
      "         -4.4950e-01,  6.7121e-02,  2.7764e-01,  8.4482e-02,  1.2456e-01,\n",
      "         -3.0520e-01, -1.9694e-01,  5.3058e-02,  9.9541e-03, -4.0822e-02,\n",
      "         -1.0541e-01,  1.6743e-01, -2.0820e-02, -1.1641e-02, -4.7713e-02,\n",
      "         -8.8681e-02,  9.6051e-02,  1.5784e-01,  9.6212e-02, -8.2319e-02,\n",
      "          1.0722e-01, -1.5116e-01,  6.8544e-03, -1.1015e-01, -1.4487e-01,\n",
      "         -2.0576e-01, -3.2406e-03, -8.2877e-01,  5.2434e-02, -9.7090e-02,\n",
      "         -1.0028e-01, -3.5258e-03,  2.1532e-01,  1.0438e-01, -4.1609e-01,\n",
      "          3.5342e-01,  2.9646e-01,  9.5543e-02,  1.3257e-01, -1.0827e-01,\n",
      "         -2.8342e-01, -1.5828e-01,  2.2266e-01, -8.3845e-02, -2.8503e-01,\n",
      "         -2.5719e-02, -2.8897e-01, -2.0676e-01,  2.7303e-01, -5.4877e-02,\n",
      "          1.9089e-01, -2.0912e-01,  2.3817e-01, -1.8409e-01,  1.0491e-01,\n",
      "          1.0817e-01, -3.3337e-01, -4.0700e-02, -1.3103e-02,  7.9504e-03,\n",
      "         -1.0245e-01, -3.3806e-02, -1.0052e-01, -8.4470e-02, -8.9528e-03,\n",
      "          1.7236e-01,  1.7747e-01, -1.8150e-01,  4.4133e+00, -2.0023e-01,\n",
      "         -2.7900e-02, -8.9535e-02,  1.8257e-01, -1.7755e-01, -1.4557e-01,\n",
      "         -2.1700e-01, -1.3200e-01, -2.0590e-01,  3.2174e-01,  1.6957e-02,\n",
      "         -4.0023e-02, -3.1821e-02,  8.6413e-02, -2.7845e-02, -9.5566e-02,\n",
      "         -3.0950e-01,  8.7540e-02, -3.4932e-01,  1.9690e-01,  1.2798e-01,\n",
      "         -4.4168e-01, -7.0200e-02, -3.5187e-01,  2.4609e-01,  1.6841e-01,\n",
      "          8.0986e-02, -1.0144e-01, -1.6638e-01,  1.1862e-01,  6.9101e-02,\n",
      "          2.6908e-01,  8.0240e-02,  2.2939e-02,  4.7101e-02,  1.9708e-02,\n",
      "          1.2492e-01,  7.1062e-02, -4.8541e-02, -1.0302e-01, -6.6811e-02,\n",
      "          1.0990e-01, -5.3981e-02, -1.2785e-01, -1.8284e-01,  1.3044e-01,\n",
      "         -2.2204e-01,  4.8251e-02, -3.2561e-01, -3.6123e-02,  3.8212e-02,\n",
      "         -1.3653e-01,  1.4124e-01, -2.8275e-01,  1.5135e-01,  1.5838e-01,\n",
      "          8.4182e-02,  3.6205e-02,  2.4256e-01,  2.7821e-02, -2.6755e-01,\n",
      "          1.1553e-01, -2.4733e-01,  1.0533e-01,  2.2848e-01, -1.0768e-01,\n",
      "          1.1646e-01,  1.5021e-02, -1.8070e-02,  1.2455e-01,  1.5764e-01,\n",
      "         -4.6228e-01,  1.1476e-01, -2.3244e-01, -1.6307e-01,  1.1228e-02,\n",
      "          1.1761e-01,  2.3892e-01, -6.8336e-02,  2.3469e-01, -4.4858e-03,\n",
      "         -2.2410e-01, -9.9361e-02, -1.3523e-01,  4.2410e-01,  2.7470e-01,\n",
      "          1.3752e-02,  3.3983e-02, -2.3675e-01, -8.4116e-02, -3.3360e-01,\n",
      "          7.9749e-02,  2.6672e-02, -3.0898e-01, -6.4169e-03,  1.3369e-02,\n",
      "         -1.7121e-01,  1.0448e-01, -9.9327e-03,  1.4435e-01,  1.1723e-01,\n",
      "          6.0756e-02,  1.2607e-01, -7.6893e-02, -6.2119e-04, -7.7294e-02,\n",
      "          4.5782e-01,  2.6824e-01, -7.7295e-02, -9.4394e-02, -1.8049e-02,\n",
      "         -7.7147e-02,  1.7105e-01,  1.6804e-01, -8.8437e-02,  1.3076e-03,\n",
      "         -2.2361e-01,  5.4922e-04,  5.0624e-01,  2.6305e-03,  6.8145e-03,\n",
      "          1.4293e-02, -4.0898e-02, -3.8939e-01,  1.5999e-01,  2.9013e-01,\n",
      "          2.7002e-01, -1.0576e-01,  1.3525e-01, -4.6365e-03,  3.5041e-04,\n",
      "         -1.1159e-01, -1.4595e-01,  7.8608e-03,  2.1101e-02, -1.5651e-01,\n",
      "          1.9197e-01,  9.6889e-03,  6.9749e-02, -1.0197e-03, -8.9265e-02,\n",
      "          1.4339e-01,  2.3515e-01, -8.1093e-02,  1.9305e-01, -2.9979e-01,\n",
      "          8.4469e-02, -4.1051e-02,  1.2041e-01,  1.1144e-01, -1.6048e-01,\n",
      "          1.9994e-01, -2.3589e-03, -1.1692e-01, -1.1375e-01, -3.5480e-01,\n",
      "         -1.4379e-01,  5.0196e-02, -1.5801e-01,  1.0474e-01, -2.1428e-01,\n",
      "          3.1607e-01, -2.1880e-01, -8.1532e-02, -2.8126e-01, -3.8997e-02,\n",
      "         -8.2536e-02,  1.3934e-01, -8.4442e-02, -3.2488e-02, -1.0611e-01,\n",
      "         -3.2835e-01,  4.5729e-01,  2.0436e-01,  6.7653e-02, -6.9744e-02,\n",
      "          1.7804e-01,  3.8313e-02,  4.4060e+00,  1.4723e-01, -2.1024e-01,\n",
      "         -2.3328e-02, -1.6299e-01, -3.1627e-01,  6.8747e-02, -4.1704e-01,\n",
      "         -1.1836e-01,  2.8072e-01,  1.0736e-01,  1.0158e-02,  1.8770e-01,\n",
      "          1.5248e-01,  1.7408e-01, -1.9022e-01,  1.5222e-01, -8.4445e-01,\n",
      "         -1.1849e-01,  5.1383e-02,  1.4356e-01, -6.7160e-02, -3.8066e-01,\n",
      "         -3.2233e-01, -2.0051e-01, -7.9802e-02,  4.0252e-01,  1.8568e-01,\n",
      "         -2.9070e-01, -6.5628e-02,  7.8343e-02,  6.6637e-02, -6.0340e-02,\n",
      "         -1.4676e-01,  1.6589e-01, -5.3843e-04,  1.2186e-01, -1.5502e-03,\n",
      "          5.5934e-02,  3.8886e-02,  1.0597e-01, -3.2743e-01, -1.1255e-01,\n",
      "          1.1447e-02, -2.7937e-01, -1.4240e-01, -1.8197e-01,  1.8689e-01,\n",
      "          5.2492e-01, -5.0149e-01, -2.4485e-01,  1.1655e-01,  5.7484e-02,\n",
      "         -2.3945e-01,  2.4707e-01, -9.6760e-02, -1.3370e-03,  8.3422e-02,\n",
      "         -2.3468e-01, -1.2777e-01,  7.8332e-02,  8.2073e-02,  6.0205e-03,\n",
      "          4.5777e-02, -1.8955e-01, -2.7800e-01,  3.5030e-02,  5.8246e-02,\n",
      "          1.5063e-01,  1.0118e-01, -9.9921e-02, -1.1025e-01,  2.9934e-01,\n",
      "          6.1498e-02, -1.5322e-01, -1.2112e-01, -1.9741e-01, -4.3913e-02,\n",
      "         -7.2048e-01,  1.6823e-01, -3.8847e-02, -2.1767e-01,  2.5686e-01,\n",
      "          5.4153e-02,  3.5862e-01,  6.4046e-02, -2.5921e-01,  3.9927e-01,\n",
      "          8.8860e-02, -5.1764e-02,  1.9842e-01,  1.4412e-02, -2.0812e-01,\n",
      "          2.0540e-01,  1.8263e-01, -3.1157e-01,  2.8921e-01,  2.1967e-01,\n",
      "          4.4334e-01,  2.3457e-01,  4.6328e-02,  2.2085e-01, -1.7905e-02,\n",
      "          2.1346e-01, -2.0530e-01, -1.8166e-01,  5.3905e-02, -2.2168e-02,\n",
      "          3.9172e-01, -1.3509e-02, -1.1961e-01, -3.8848e-01,  3.1173e-01,\n",
      "         -2.1800e-01, -1.4337e-01,  2.8928e-01,  1.0779e-01,  1.8246e-01,\n",
      "         -6.9454e-02,  6.4073e-02, -1.2852e-01, -2.6249e-02, -2.5918e-01,\n",
      "         -2.2305e-01,  9.6617e-02, -1.3845e-01, -1.9797e-01, -5.8292e-02,\n",
      "         -4.6925e-02,  2.3645e-01,  1.0755e-01, -1.1341e-01,  1.6930e-02,\n",
      "         -1.5335e-01, -3.3458e-02, -3.2914e-01, -1.0079e-01,  8.7582e-02,\n",
      "         -3.3781e-01,  8.9560e-02, -9.6948e-02, -1.0715e-01,  1.7797e-01,\n",
      "         -1.2337e-01,  1.1991e-01, -1.4159e-01, -7.7419e-03,  1.3633e-01,\n",
      "          1.7927e-02, -9.8370e-02, -5.4132e-02,  7.9220e-02,  1.5602e-01,\n",
      "         -1.6659e-01, -1.3770e-01, -3.4433e-01, -6.7015e-02,  7.4209e-02,\n",
      "         -2.0491e-02, -2.0849e-02,  1.6380e-01, -2.8462e-01,  4.1734e-02,\n",
      "         -5.4773e-01,  4.2349e-01, -1.7994e-01, -2.0069e-02, -1.2305e-01,\n",
      "         -1.4283e-01, -1.1269e-01,  1.6282e-02, -1.9070e-01, -1.0776e-01,\n",
      "         -1.4905e-01,  1.6197e-01,  1.7322e-01,  6.9149e-03,  1.6601e-01,\n",
      "          5.5289e-01, -1.0244e-02, -3.2872e-01,  9.1241e-01,  1.0433e-01,\n",
      "         -1.3183e-01, -1.6200e-01, -1.9512e-01, -4.0945e-01,  1.1184e-01,\n",
      "         -1.4976e-02,  8.9711e-03, -4.4966e-02,  4.0536e-01,  3.8843e-01,\n",
      "          1.2832e-02, -3.2944e-01,  2.7267e-01, -4.1479e-03,  9.8242e-02,\n",
      "         -1.1272e-01, -9.7631e-02]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def get_caption_feature_vector(captions):\n",
    "    \"\"\"\n",
    "    Generates an overall feature vector for a list of captions by averaging the CLIP feature vectors.\n",
    "\n",
    "    Args:\n",
    "        captions (list): List of caption strings.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Averaged feature vector representing the captions.\n",
    "    \"\"\"\n",
    "    # Function to encode a caption into a feature vector using CLIP\n",
    "    def encode_caption(caption):\n",
    "        text_inputs = clip_processor(text=[caption], return_tensors=\"pt\", padding=True)\n",
    "        text_features = clip_model.get_text_features(**text_inputs)\n",
    "        return text_features\n",
    "\n",
    "    # Encode each caption and store the vectors\n",
    "    caption_vectors = [encode_caption(caption) for caption in captions]\n",
    "\n",
    "    # Calculate the final feature vector by averaging the caption vectors\n",
    "    overall_feature_vector = torch.mean(torch.stack(caption_vectors), dim=0)\n",
    "    \n",
    "    return overall_feature_vector\n",
    "\n",
    "# Example usage\n",
    "captions = [\n",
    "    \"3 mountain climbers are on their way to the mountain.\",\n",
    "    \"A steep mountainside covered with snow in patches.\",\n",
    "    \"A mountain climber climbing a snow-covered mountain slope.\",\n",
    "    \"A mountain climber standing and resting near the top of the mountain.\",\n",
    "    \"A view from the campground while climbing the mountain peak.\"\n",
    "]\n",
    "\n",
    "# Call the function and get the overall feature vector\n",
    "overall_feature_vector_ground_truth = get_caption_feature_vector(captions)\n",
    "print(\"Overall feature vector for the ground truths:\", overall_feature_vector_ground_truth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity between image feature vector and the story feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity score between image and story feature vectors: 0.7565784454345703\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming overall_feature_vector_image and overall_feature_vector_story are your two feature vectors\n",
    "def calculate_cosine_similarity(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two feature vectors.\n",
    "\n",
    "    Args:\n",
    "        vector1 (torch.Tensor): First feature vector.\n",
    "        vector2 (torch.Tensor): Second feature vector.\n",
    "\n",
    "    Returns:\n",
    "        float: Cosine similarity score between -1 and 1.\n",
    "    \"\"\"\n",
    "    # Flatten to ensure 1D tensors\n",
    "    vector1 = vector1.flatten()\n",
    "    vector2 = vector2.flatten()\n",
    "    \n",
    "    # Normalize each vector\n",
    "    vector1 = vector1 / vector1.norm()\n",
    "    vector2 = vector2 / vector2.norm()\n",
    "    \n",
    "    # Calculate the cosine similarity using dot product\n",
    "    similarity = torch.dot(vector1, vector2)\n",
    "    return similarity.item()\n",
    "\n",
    "# Example usage:\n",
    "cosine_similarity_score1 = calculate_cosine_similarity(overall_feature_vector_image, overall_feature_vector_story)\n",
    "print(\"Cosine similarity score between image and story feature vectors:\", cosine_similarity_score1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cosine similarity between the image feature vector and the ground truth feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity score between image and ground truth feature vectors: 0.8385794162750244\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming overall_feature_vector_image and overall_feature_vector_ground_truth are your two feature vectors\n",
    "def calculate_cosine_similarity(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two feature vectors.\n",
    "\n",
    "    Args:\n",
    "        vector1 (torch.Tensor): First feature vector.\n",
    "        vector2 (torch.Tensor): Second feature vector.\n",
    "\n",
    "    Returns:\n",
    "        float: Cosine similarity score between -1 and 1.\n",
    "    \"\"\"\n",
    "    # Flatten to ensure 1D tensors\n",
    "    vector1 = vector1.flatten()\n",
    "    vector2 = vector2.flatten()\n",
    "    \n",
    "    # Normalize each vector\n",
    "    vector1 = vector1 / vector1.norm()\n",
    "    vector2 = vector2 / vector2.norm()\n",
    "    \n",
    "    # Calculate the cosine similarity using dot product\n",
    "    similarity = torch.dot(vector1, vector2)\n",
    "    return similarity.item()\n",
    "\n",
    "# Example usage:\n",
    "cosine_similarity_score2 = calculate_cosine_similarity(overall_feature_vector_ground_truth, overall_feature_vector_story)\n",
    "print(\"Cosine similarity score between image and ground truth feature vectors:\", cosine_similarity_score2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the rouge score between the ground truths and the final story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 (Unigram) Score: Score(precision=0.13425925925925927, recall=0.5918367346938775, fmeasure=0.2188679245283019)\n",
      "ROUGE-2 (Bigram) Score: Score(precision=0.03255813953488372, recall=0.14583333333333334, fmeasure=0.0532319391634981)\n",
      "ROUGE-L (Longest Common Subsequence) Score: Score(precision=0.07407407407407407, recall=0.32653061224489793, fmeasure=0.12075471698113208)\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ground_truths = [\n",
    "    \"3 mountain climbers are on their way to the mountain.\",\n",
    "    \"A steep mountainside covered with snow in patches.\",\n",
    "    \"a mountain climber climbing a snow-covered mountain slope\",\n",
    "    \"a mountain climber standing and resting near the top of the mountain\",\n",
    "    \"A view from the campground while climbing the mountain peak.\"\n",
    "]\n",
    "\n",
    "# Define the generated story (candidate text)\n",
    "story_text = \"\"\"\n",
    "Once upon a time, in the heart of the mountains, the group was stopped by a man. \"He's a member of this group. He's called a soldier of an army. You can tell he's an enemy of mine. So, why don't you get a look at him?\" he stared at the snowman, who had no idea where he was. The man had a strange look on his face, and he seemed to be a bit of a weirdo. However, he didn't say anything to the other snow he climbed up to a mountain, his right hand being on the mountain's top. A snowflake-shaped, red-colored mountain that was very steep, like a cave. It was about three meters tall, with a peak that could be reached he looked at his snow-covered face. There was a faint smile on its face as it looked down at a white-haired man, wearing a snow mask. After a moment, it began to grow a little taller. His face was dark red the man's face grew even more, making it seem as though he had been a very powerful and powerful person. As he walked, a small voice spoke from behind him, \"I heard you're coming. Why are you here?\"\n",
    "\"\"\"\n",
    "\n",
    "# Concatenate all captions into a single reference text for comparison\n",
    "reference_text = \" \".join(ground_truths)\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Calculate ROUGE score\n",
    "scores = scorer.score(reference_text, story_text)\n",
    "\n",
    "# Display the results\n",
    "print(\"ROUGE-1 (Unigram) Score:\", scores['rouge1'])\n",
    "print(\"ROUGE-2 (Bigram) Score:\", scores['rouge2'])\n",
    "print(\"ROUGE-L (Longest Common Subsequence) Score:\", scores['rougeL'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The low precision suggests that the generated story includes many new words, while the high recall indicates it captures a decent portion of the important words from the ground truths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# below this is bullshit have to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Reward: 1.0164789308547975\n"
     ]
    }
   ],
   "source": [
    "def compute_reward(r_score, r_topic_cv, r_topic_cl, lambda_val, gamma_val, eta_val):\n",
    "    \"\"\"\n",
    "    Compute the reward function r(y_i) using ROUGE F1 score and cosine similarities.\n",
    "\n",
    "    Parameters:\n",
    "    - r_score (float): ROUGE F1 score (or BLEU score, as applicable).\n",
    "    - r_topic_cv (float): Cosine similarity for vision-based topics.\n",
    "    - r_topic_cl (float): Cosine similarity for language-based topics.\n",
    "    - lambda_val (float): Weight for ROUGE F1 score.\n",
    "    - gamma_val (float): Weight for topic-cv similarity.\n",
    "    - eta_val (float): Weight for topic-cl similarity.\n",
    "\n",
    "    Returns:\n",
    "    - float: Computed reward r(y_i).\n",
    "    \"\"\"\n",
    "    reward = (lambda_val * r_score) + (gamma_val * r_topic_cv) + (eta_val * r_topic_cl)\n",
    "    return reward\n",
    "\n",
    "# Example ROUGE F1 score and cosine similarities\n",
    "rouge_f1 = 0.2189  # ROUGE F1 score\n",
    "r_topic_cv = 0.8385794162750244  # Example cosine similarity for topic-cv\n",
    "r_topic_cl = 0.7565784454345703  # Example cosine similarity for topic-cl\n",
    "\n",
    "# Assign weights\n",
    "lambda_val = 1.0  # Weight for ROUGE F1 score\n",
    "gamma_val = 0.5   # Weight for topic-cv similarity\n",
    "eta_val = 0.5     # Weight for topic-cl similarity\n",
    "\n",
    "# Compute reward\n",
    "reward = compute_reward(rouge_f1, r_topic_cv, r_topic_cl, lambda_val, gamma_val, eta_val)\n",
    "print(\"Computed Reward:\", reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 96\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ground truths and story text\n",
    "ground_truths = [\n",
    "    \"3 mountain climbers are on their way to the mountain.\",\n",
    "    \"A steep mountainside covered with snow in patches.\",\n",
    "    \"a mountain climber climbing a snow-covered mountain slope\",\n",
    "    \"a mountain climber standing and resting near the top of the mountain\",\n",
    "    \"A view from the campground while climbing the mountain peak.\"\n",
    "]\n",
    "story_text = \"\"\"\n",
    "Once upon a time, in the heart of the mountains, the group was stopped by a man. \n",
    "\"He's a member of this group. He's called a soldier of an army. You can tell he's an enemy of mine. \n",
    "So, why don't you get a look at him?\" he stared at the snowman, who had no idea where he was. \n",
    "The man had a strange look on his face, and he seemed to be a bit of a weirdo. \n",
    "However, he didn't say anything to the other snow he climbed up to a mountain, his right hand being on the mountain's top. \n",
    "\"\"\"\n",
    "\n",
    "# Combine ground truths and story text\n",
    "all_texts = \" \".join(ground_truths) + \" \" + story_text\n",
    "\n",
    "# Tokenize and count unique words\n",
    "tokens = word_tokenize(all_texts)\n",
    "vocab = Counter(tokens)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # Embedding layer\n",
    "        rnn_out, _ = self.rnn(embedded)  # LSTM layer\n",
    "        output = self.fc(rnn_out)  # Fully connected layer\n",
    "        return output\n",
    "\n",
    "# Example initialization of the model\n",
    "input_size = vocab_size  # Vocabulary size for embedding\n",
    "hidden_size = 256        # Size of the hidden layers\n",
    "output_size = vocab_size # Output vocabulary size\n",
    "model = Seq2SeqModel(input_size, hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Inputs: tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9,  2, 10],\n",
      "        [11,  2, 19, 20, 11, 21,  2, 22,  0,  0,  0]])\n",
      "Targets: tensor([[34, 35, 11, 36, 37, 17,  9, 38, 28,  9, 39, 37,  9, 40, 41, 42, 43, 11,\n",
      "         44, 10, 45, 46, 47, 11, 48, 28, 49, 40, 10, 46, 47, 50, 11, 51, 28, 52,\n",
      "         53, 10, 54, 55, 56, 46, 47, 52, 57, 28, 58, 10, 59, 37, 60, 61, 62, 54,\n",
      "         63, 11, 64, 65, 66, 67, 68, 46, 69, 65,  9, 70, 37, 71, 72, 73, 74, 75,\n",
      "         46, 41, 10,  9, 44, 72, 11, 76, 64,  5, 77, 78, 37, 24, 46, 79,  8, 80,\n",
      "         11, 81, 28, 11, 82, 10, 83, 37, 46, 84, 62, 85, 86,  8,  9, 87, 16, 46,\n",
      "         88, 89,  8, 11,  2, 37, 77, 90, 91, 92,  5,  9,  2, 47, 27, 10],\n",
      "        [34, 35, 11, 36, 37, 17,  9, 38, 28,  9, 39, 37,  9, 40, 41, 42, 43, 11,\n",
      "         44, 10, 45, 46, 47, 11, 48, 28, 49, 40, 10, 46, 47, 50, 11, 51, 28, 52,\n",
      "         53, 10, 54, 55, 56, 46, 47, 52, 57, 28, 58, 10, 59, 37, 60, 61, 62, 54,\n",
      "         63, 11, 64, 65, 66, 67, 68, 46, 69, 65,  9, 70, 37, 71, 72, 73, 74, 75,\n",
      "         46, 41, 10,  9, 44, 72, 11, 76, 64,  5, 77, 78, 37, 24, 46, 79,  8, 80,\n",
      "         11, 81, 28, 11, 82, 10, 83, 37, 46, 84, 62, 85, 86,  8,  9, 87, 16, 46,\n",
      "         88, 89,  8, 11,  2, 37, 77, 90, 91, 92,  5,  9,  2, 47, 27, 10]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Data\n",
    "ground_truths = [\n",
    "    \"3 mountain climbers are on their way to the mountain.\",\n",
    "    \"A steep mountainside covered with snow in patches.\",\n",
    "    \"a mountain climber climbing a snow-covered mountain slope\",\n",
    "    \"a mountain climber standing and resting near the top of the mountain\",\n",
    "    \"A view from the campground while climbing the mountain peak.\"\n",
    "]\n",
    "story_text = \"\"\"\n",
    "Once upon a time, in the heart of the mountains, the group was stopped by a man. \n",
    "\"He's a member of this group. He's called a soldier of an army. You can tell he's an enemy of mine. \n",
    "So, why don't you get a look at him?\" he stared at the snowman, who had no idea where he was. \n",
    "The man had a strange look on his face, and he seemed to be a bit of a weirdo. \n",
    "However, he didn't say anything to the other snow he climbed up to a mountain, his right hand being on the mountain's top. \n",
    "\"\"\"\n",
    "\n",
    "# Combine all ground truths into a single string for vocabulary building\n",
    "all_text = \" \".join(ground_truths) + \" \" + story_text\n",
    "\n",
    "# Tokenize and build vocabulary\n",
    "tokens = word_tokenize(all_text.lower())  # Lowercase and tokenize\n",
    "vocab_counter = Counter(tokens)  # Count occurrences of each token\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(vocab_counter.items(), start=1)}  # Assign index starting from 1\n",
    "vocab[\"<PAD>\"] = 0  # Add <PAD> token for padding\n",
    "\n",
    "# Convert tokens to indices\n",
    "def text_to_indices(text, vocab):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize\n",
    "    return [vocab[token] for token in tokens if token in vocab]  # Convert to indices\n",
    "\n",
    "# Convert ground truths and story text to indices\n",
    "input_sequences = [text_to_indices(gt, vocab) for gt in ground_truths]\n",
    "target_sequence = text_to_indices(story_text, vocab)\n",
    "\n",
    "# Dataset Definition\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, input_sequences, target_sequence, vocab):\n",
    "        self.input_sequences = input_sequences  # List of input sequences\n",
    "        self.target_sequence = target_sequence  # Single target sequence\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_tensor = torch.tensor(self.input_sequences[idx], dtype=torch.long)\n",
    "        target_tensor = torch.tensor(self.target_sequence, dtype=torch.long)\n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "# Collate Function for Padding\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=vocab[\"<PAD>\"])\n",
    "    padded_targets = pad_sequence([targets[0]] * len(inputs), batch_first=True, padding_value=vocab[\"<PAD>\"])\n",
    "    return padded_inputs, padded_targets\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = Seq2SeqDataset(input_sequences, target_sequence, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Check the dataloader output\n",
    "for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(\"Inputs:\", inputs)\n",
    "    print(\"Targets:\", targets)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape mismatch: Outputs (torch.Size([24, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([22, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([9, 93])) vs Targets (torch.Size([124]))\n",
      "Epoch 1, MLE Loss: 0\n",
      "Shape mismatch: Outputs (torch.Size([22, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([24, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([8, 93])) vs Targets (torch.Size([124]))\n",
      "Epoch 2, MLE Loss: 0\n",
      "Shape mismatch: Outputs (torch.Size([22, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([24, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([9, 93])) vs Targets (torch.Size([124]))\n",
      "Epoch 3, MLE Loss: 0\n",
      "Shape mismatch: Outputs (torch.Size([24, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([18, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([11, 93])) vs Targets (torch.Size([124]))\n",
      "Epoch 4, MLE Loss: 0\n",
      "Shape mismatch: Outputs (torch.Size([24, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([22, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([11, 93])) vs Targets (torch.Size([124]))\n",
      "Epoch 5, MLE Loss: 0\n",
      "Shape mismatch: Outputs (torch.Size([22, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([24, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([9, 93])) vs Targets (torch.Size([124]))\n",
      "Epoch 6, MLE Loss: 0\n",
      "Shape mismatch: Outputs (torch.Size([22, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([24, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([8, 93])) vs Targets (torch.Size([124]))\n",
      "Epoch 7, MLE Loss: 0\n",
      "Shape mismatch: Outputs (torch.Size([24, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([22, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([11, 93])) vs Targets (torch.Size([124]))\n",
      "Epoch 8, MLE Loss: 0\n",
      "Shape mismatch: Outputs (torch.Size([24, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([22, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([11, 93])) vs Targets (torch.Size([124]))\n",
      "Epoch 9, MLE Loss: 0\n",
      "Shape mismatch: Outputs (torch.Size([24, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([22, 93])) vs Targets (torch.Size([248]))\n",
      "Shape mismatch: Outputs (torch.Size([11, 93])) vs Targets (torch.Size([124]))\n",
      "Epoch 10, MLE Loss: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Seq2Seq Model\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch_size, seq_length, hidden_size)\n",
    "        output, _ = self.rnn(x)  # (batch_size, seq_length, hidden_size)\n",
    "        logits = self.fc(output)  # (batch_size, seq_length, vocab_size)\n",
    "        return logits\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<PAD>\"])  # Ignore padding tokens\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Collate function to pad both inputs and targets\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=vocab[\"<PAD>\"])\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=vocab[\"<PAD>\"])  # Pad targets too\n",
    "    return padded_inputs, padded_targets\n",
    "\n",
    "# Pretraining with MLE\n",
    "num_epochs = 10  # Number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:  # Use the defined dataloader\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)  # Shape: (batch_size, seq_length, vocab_size)\n",
    "        \n",
    "        # Get batch size and sequence length from outputs\n",
    "        batch_size, seq_len, vocab_size = outputs.shape\n",
    "        \n",
    "        # Flatten outputs and targets correctly\n",
    "        outputs = outputs.view(-1, vocab_size)  # Flatten to (batch_size * seq_len, vocab_size)\n",
    "        targets = targets.view(-1)  # Flatten to (batch_size * seq_len,)\n",
    "        \n",
    "        # Ensure that both outputs and targets have the same length after reshaping\n",
    "        if outputs.shape[0] != targets.shape[0]:\n",
    "            print(f\"Shape mismatch: Outputs ({outputs.shape}) vs Targets ({targets.shape})\")\n",
    "            continue  # Skip this batch if shapes are not aligned\n",
    "        \n",
    "        # Compute MLE loss (CrossEntropyLoss)\n",
    "        loss = criterion(outputs, targets)  # Apply loss function\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, MLE Loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
